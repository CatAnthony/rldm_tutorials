{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "import numpy as np\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Setup\n",
    "In this notebook, we present a pymdptoolbox implementation of the gridworld environment defined in Lesson 2. To define an MDP in the mdptoolbox, we need to construct numpy arrays for both the transition matrix and reward matrix. \n",
    "\n",
    "We are going to use the transitions defined in Lesson 2, Video 5 \"Quiz: The World - 2\":\n",
    "<img src=\"images/theworld.png\",width=240,height=240>\n",
    "\n",
    "And the rewards defined in Lesson 2, Video 12 \"Quiz: More About Rewards - 3\":\n",
    "<img src=\"images/rewards.png\",width=240,height=240>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#reward inputs\n",
    "r_s = +2 #most states\n",
    "r_g = +1 #good terminal state\n",
    "r_b = -1 #bad terminal state\n",
    "\n",
    "#transition inputs\n",
    "p_intended = .8\n",
    "p_opposite = 0.0\n",
    "p_right = .1\n",
    "p_left = .1\n",
    "\n",
    "#create_mdp can be found in helpers.py\n",
    "T , R = create_mdp([r_s, r_g, r_b], \\\n",
    "                   [p_intended, p_opposite, p_right, p_left])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Value Iteration\n",
    "Once we have both matrices, we are going to run value iteration using the mdptoolbox.\n",
    "\n",
    "Note: we've defined the transition matrix T such that up is 0, right is 1, down is 2 and left is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: check conditions of convergence. With no discount, convergence can not be assumed.\n",
      "[[0 0 3 0]\n",
      " [0 0 3 0]\n",
      " [1 0 3 2]]\n"
     ]
    }
   ],
   "source": [
    "#create object, undiscounted for now\n",
    "vi = mdptoolbox.mdp.ValueIteration(T, R, discount=1)\n",
    "\n",
    "#run value iteration silently\n",
    "vi.setSilent()\n",
    "vi.run()\n",
    "\n",
    "#print policy found by value iteration\n",
    "print(np.array(vi.policy).reshape((3,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Finding Q-values\n",
    "However, this doesn't tell us when the choice of action doesn't matter. We need to look at the Q (state-action) values to see if all actions for a given state have the same value. \n",
    "\n",
    "To explore this we will use the function get_q_values (found in helpers.py.) This function outputs a policy where \"-1\" denotes that all actions have the same Q-value (precision: the mdp object's epsilon value).\n",
    "\n",
    "With verbose on, Q-values for each state are also presented. State numbers are as follows:\n",
    "\n",
    "|  |  |   |   |\n",
    "|---|---|----|----|\n",
    "| 0 | 1 | 2  | 3  |  \n",
    "| 4 | 5 | 6  | 7  |\n",
    "| 8 | 9 | 10 | 11 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values:\n",
      "\n",
      "State 0: [ 2002.  2002.  2002.  2002.]\n",
      "State 1: [ 2002.  2002.  2002.  2002.]\n",
      "State 2: [ 1902.  1202.  1902.  2002.]\n",
      "State 3: [ 1001.  1001.  1001.  1001.]\n",
      "State 4: [ 2002.  2002.  2002.  2002.]\n",
      "State 5: [ 100100.  100100.  100100.  100100.]\n",
      "State 6: [ 1702.  -398.  1702.  2002.]\n",
      "State 7: [-1001. -1001. -1001. -1001.]\n",
      "State 8: [ 2002.  2002.  2002.  2002.]\n",
      "State 9: [ 2002.  2002.  2002.  2002.]\n",
      "State 10: [ 2002.  2002.  2002.  2002.]\n",
      "State 11: [ -398.  1702.  2002.  1702.]\n",
      "\n",
      "=====================\n",
      "Policy:\n",
      "\n",
      "[[-1 -1  3 -1]\n",
      " [-1 -1  3 -1]\n",
      " [-1 -1 -1  2]]\n"
     ]
    }
   ],
   "source": [
    "#print policy with -1 where all actions have same Q value\n",
    "#in verbose mode we will also see the Q values themselves\n",
    "vi.setVerbose()\n",
    "print(\"Q-values:\\n\")\n",
    "policy = get_q_values(vi).reshape((3,4))\n",
    "print(\"\\n=====================\\nPolicy:\\n\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "State 10's Q-values are all the same. But what if we:\n",
    "\n",
    "1. Change the transition matrix by making p_intended = .7 and p_opposite = .1?\n",
    "2. Take it further and make p_intended = .7999 and p_opposite = .0001?\n",
    "\n",
    "Feel free to play around with this notebook and the functions in helpers.py to get more of an intuition about MDPs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
